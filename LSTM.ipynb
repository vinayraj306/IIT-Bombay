{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies numpy and keras\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading text file and creating character to integer mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = \"/macbeth.txt\"\n",
    "\n",
    "text = (open(filename).read()).lower()\n",
    "\n",
    "# mapping characters with integers\n",
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for i, c in enumerate (unique_chars):\n",
    "    char_to_int.update({c: i})\n",
    "    int_to_char.update({i: c})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing input and output dataset\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(0, len(text) - 50, 1):\n",
    "    sequence = text[i:i + 50]\n",
    "    label =text[i + 50]\n",
    "    X.append([char_to_int[char] for char in sequence])\n",
    "    Y.append(char_to_int[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reshaping of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping, normalizing and one hot encoding\n",
    "X_modified = numpy.reshape(X, (len(X), 50, 1))\n",
    "X_modified = X_modified / float(len(unique_chars))\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LSTM network expects the input to be in the form [samples, time steps, features] where samples is the number of data points we have, time steps is the number of time-dependent steps that are there in a single data point, features refers to the number of variables we have for the corresponding true value in Y. We then scale the values in X_modified between 0 to 1 and one hot encode our true values in Y_modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential model which is a linear stack of layers is used. The first layer is an LSTM layer with 300 memory units and it returns sequences. This is done to ensure that the next LSTM layer receives sequences and not just randomly scattered data. A dropout layer is applied after each LSTM layer to avoid overfitting of the model. Finally, we have the last layer as a fully connected layer with a ‘softmax’ activation and neurons equal to the number of unique characters, because we need to output one hot encoded result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting the model and generating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "model.fit(X_modified, Y_modified, epochs=1, batch_size=30)\n",
    "\n",
    "# picking a random seed\n",
    "start_index = numpy.random.randint(0, len(X)-1)\n",
    "new_string = X[start_index]\n",
    "\n",
    "# generating characters\n",
    "for i in range(50):\n",
    "    x = numpy.reshape(new_string, (1, len(new_string), 1))\n",
    "    x = x / float(len(unique_chars))\n",
    "\n",
    "    #predicting\n",
    "    pred_index = numpy.argmax(model.predict(x, verbose=0))\n",
    "    char_out = int_to_char[pred_index]\n",
    "    seq_in = [int_to_char[value] for value in new_string]\n",
    "    print(char_out)\n",
    "\n",
    "    new_string.append(pred_index)\n",
    "    new_string = new_string[1:len(new_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The model is fit over 100 epochs, with a batch size of 30. We then fix a random seed (for easy reproducibility) and start generating characters. The prediction from the model gives out the character encoding of the predicted character, it is then decoded back to the character value and appended to the pattern.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
